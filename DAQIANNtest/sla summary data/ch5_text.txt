COGNITION

Cognition refers to how information is processed and learned by the human mind (the term comes from the Latin verb cognoscere, ‘to get to know’). SLA researchers interested in cognition study what it takes to ‘get to know’ an additional language well enough to use it ﬂuently in comprehension and production. We are far from a satisfactory understanding of second language as a form of cognition, however. This is because our capacities to investigate the relevant questions are shaped by the pace at which new theories and methods to inspect the workings of human minds and brains become available (typically in neighbouring disciplines) and the rate at which SLA researchers become conversant in them. In this chapter, more than in any other, I will make frequent reference to relevant L1 research and point at areas where future attention by SLA researchers will be needed.
It is also important to realize that, in cognitive research, the relevant behavioural and neurobiological evidence falls in the order of a few hundred milliseconds to a few seconds, or it consists of larger-scope performance that nevertheless lasts a few minutes to a few hours at the most. This is in sharp contrast with many of the data on language learning SLA researchers normally consider, which involve stretches of discourse, multi-turn interactions with human interlocutors, extended texts, referential and social meaning, and even years of studying, using or living with an L2. Thus, the differences in grain size, temporal and ontological, of the various phenomena that are brought together into cognitive explorations of L2 learning are puzzling.
In this chapter you will learn about cognitive SLA theories and constructs that have been developed to explain the nature of second language as a form of cognition. The theories can be broadly classiﬁed into traditional information processing, which has dominated SLA theorizing and research since the mid-1980s, and emergentism, which is a development of the late 1990s that grew out from the former. A central preoccupation in SLA research on cognition is with memory and attention in L2 learning.

5.1	INFORMATION PROCESSING IN PSYCHOLOGY AND SLA	

Information processing emerged in the ﬁeld of psychology in the 1970s, out of the so-called cognitive revolution of the late 1950s. Initially a reaction against behaviourist theories that could only offer stimulus–response explanations for human learning, it became the dominant psychological paradigm of the last third of the twentieth century. In a nutshell, the human mind is viewed as a symbolic processor that constantly engages in mental processes. These mental processes operate on mental representations and intervene between input (whatever data get into the symbolic processor, the mind) and output (whatever the results of performance are). Performance, rather than behaviour, is a key word in information processing theories. This is because inferences about mental processes can only be made by inspecting what is observable during processing while performing tasks, rather than by inspecting external behaviour in response to stimuli, as behaviourists used to do.
Several key assumptions made by information processing psychologists have been embraced in current SLA research about cognition. First, the human cognitive architecture is made of representation and access. Second, mental processing is dual, comprised of two different kinds of computation: automatic or ﬂuent (unconscious) and voluntary or controlled (conscious). Third, cognitive resources such as attention and memory are limited. Let us unpack each principle in some detail for a better understanding of what information processing stands for.
Information processing theories distinguish between representation (or knowledge) and access (or processing). Bialystok and Sharwood Smith (1985) used a library metaphor to explain this distinction to their SLA audience: ‘knowing what is in the library, plus how the contents are classiﬁed and related to one another, must be distinguished from retrieving desired information from the books at a given time’ (p. 105). Linguistic representation is comprised of three kinds of knowledge: grammatical, lexical and schematic or world-related. New L2 knowledge is stored in the mind and has to be accessed and retrieved every time it is needed for use in comprehension or production.
Access entails the activation or use of relevant knowledge via two different mechanisms known as automatic and controlled processing. Canadian language psychologist Norman Segalowitz (2003) compares the two modes of processing to the difference between an automatic and a standard shift car: ‘an automatic shift car changes gears without deliberate intervention by the driver, in contrast to a standard shift car which requires the driver to perform a manual operation’ (p. 383). Unlike cars, however, which are built by manufacturers to function as either automatic or standard shift from the outset, human cognition is supported by both automatic and controlled processing. Information processing psychologists believe that all human perception and action, as well as all thoughts and feelings, result from the interaction of these two kinds of processing.
Automatic processes require small effort and take up few cognitive resources, and therefore many automatic processing routines can run in parallel. During automatic processing, cognitive activation is triggered bottom up by exogenous sources in the environment (something outside the processor, that is, some aspect of the data in the input or environment). By contrast, controlled processing is activated by top-down, endogenous sources (by something inside the processor, that is, by voluntary, goal-directed motivation in the individual’s mind), and it is handled by what we call the central executive. We summon controlled processing when we intentionally set out to control behaviour, for example, when no automatic routines have been learned yet because the problem is new (as in a new language) or in the face of some kind of problem encountered during automatic processing (as when surrounding noise makes us strive to understand the few disconnected sounds that we can gather from our interlocutor). In such cases, we let our central executive system intervene to ‘control’ the processing task.
Controlled processes therefore allow us self-regulation, but they require a lot more effort and cognitive resources than automatic processes, and thus cannot operate in parallel; they are serial. For this reason, controlled processing is subject to a bottleneck effect. When we voluntarily attend to one thing, we need to block out the rest. If several demands are competing for controlled processing, they will be prioritized and certain processes will wait in line, so to speak, while only one is being executed. This is what we call a limited capacity model of information processing. The model predicts that performance that draws on controlled processing is more variable and more vulnerable to stressors than performance that draws on automatic processing. Therefore, a widely employed method in the study of automaticity is the dual-task condition, where the researcher creates processing stress by asking participants to carry out two tasks simultaneously, a primary task and a distracting task. Under this dual-task pressure, because the distracting task consumes attention away from the primary task, performance on the main task may become variable and vulnerable. If this happens, it is taken as evidence that the participant is relying on more controlled processing and therefore has not yet reached automatization on the performance called by the primary task.

5.2	THE POWER OF PRACTICE: PROCEDURALIZATION AND AUTOMATICITY	

A particular kind of information processing theory, called skill acquisition theory, has been fruitful in guiding SLA efforts since the mid-1980s (e.g. Bialystok and Sharwood Smith, 1985; McLaughlin, 1987). The most inﬂuential version has been adopted from the early formulations of cognitive psychologist John Anderson’s Adaptive Control of Thought theory (Anderson, 1983), although his most recent version of the theory goes well beyond traditional information processing notions (Anderson, 2007).
Skill acquisition theory deﬁnes learning as the gradual transformation of performance from controlled to automatic. This transformation happens through relevant practice over many trials, which enables controlled processes gradually to be withdrawn during performance and automatic processes to take over the same performance. The process has been called proceduralization or automatization and entails the conversion of declarative or explicit knowledge (or ‘knowledge that’) into procedural or implicit knowledge (or ‘knowledge how’). It is important to realize that the learning of skills is assumed to start with the explicit provision of relevant declarative knowledge. Thus, L2 learners (particularly instructed learners) begin with explanations explicitly presented by their teachers or in textbooks and, through practice, this knowledge can hopefully convert into ability for use, or implicit-procedural knowledge made up of automatic routines.
How does practice work? It helps proceduralization of new knowledge by allowing the establishment and strengthening of corresponding links in long-term memory. The more this knowledge is accessed via practice, the easier it will become to access it without effort and without the involvement of the central executive at a future time. However, the power of practice is not constant over time. There is a well-known power law of learning, by which practice will at some point yield no large returns in terms of improvement, because optimal performance has been reached (Ellis and Schmidt, 1998). In addition, proceduralization is skill-speciﬁc. Therefore, practice that focuses on L2 production should help automatize production and practice that focuses on L2 comprehension should help automatize comprehension (DeKeyser, 1997). The ﬁnal outcome of the gradual process of proceduralization or automatization is automaticity, which is deﬁned as automatic performance that draws on implicit-procedural knowledge and is reﬂected in ﬂuent comprehension and production and in lower neural activation patterns (Segalowitz, 2003).
Two misinterpretations of skill acquisition tenets are common: (a) that automaticity is simply accelerated or speedy behaviour; and (b) that L2 learners simply accumulate rules that they practise until they can use them automatically. Much to the contrary, Segalowitz (2003) discusses in depth how skilled performance cannot be understood in terms of sheer speed alone, and that instead a qualitative change is reached once performance is automatized. Likewise, rather than simply accumulating rules, prolonged and repeated practice changes the knowledge representation itself by making the stored knowledge become more elaborated and well speciﬁed, or more analysed, as Canadian psychologist Ellen Bialystok (2001) has called it. This happens via processes of accretion, tuning and restructuring of knowledge (discussed by McLaughlin and Heredia, 1996; see also Chapter 6, section 6.4). In other words, by the time they become automatized, rules may be just different from the declarative rules that were initially committed to memory.
How would anyone be able to study all these abstract principles of skill acquisition theory, when it comes to L2 learning? In the next section, I will walk you through a study by DeKeyser (1997) that embodies all these principles. The study is an exemplary full-blown effort to document the time course of proceduralization during second language learning.

5.3	AN EXEMPLARY STUDY OF SKILL ACQUISITION THEORY IN SLA: DEKEYSER (1997)	

Robert DeKeyser (1997) investigated many of the predictions of information processing and cognitive skill theory in a clever and complicated study. DeKeyser recruited 61 college student volunteers and taught them a miniature language that he called Autopractan. He did so over 11 weeks and 22 one-hour sessions, using picture-and-sentence exercises delivered through a computer program. Autopractan comprised a small set of 16 nouns and 16 verbs and was designed to behave like natural languages, with morphological markings for gender, number and case, and the possibility of omitting the subject and having ﬂexible word order (that is, Autopractan was a null-subject language with rich morphology). One challenge in this kind of study is that participants may not see the beneﬁts of trying hard to study a language that they know is artiﬁcial, and therefore of no use to them outside the experiment. In an effort to address this problem, the researcher told the volunteers their monetary compensation for participating in the study would vary depending on their scores during the experiment. The difference was a modest $8 per hour for top scores versus $6 per hour for bottom scores, but this ought to have been enough of an incentive for undergraduate students in the mid-1990s!
The ﬁrst phase was provision of explicit, declarative knowledge. It involved presenting all vocabulary and grammar rules of Autopractan and having participants learn them well over the ﬁrst six sessions (about three weeks). The second phase was practice. It was designed to support proceduralization, or the transformation of performance from controlled to automatic. It involved different things for different groups, but it always took 15 sessions (eight weeks) and exactly the same total number of exercises for everyone. Finally, the last session in the study (session number 22) was devoted to testing participants on four Autopractan rules via comprehension and production test items.
How did the practice phase vary across conditions? One group (n = 21) practised Autopractan rules 1 and 2 only through comprehension and rules 3 and 4 only through production. A second group (n = 20) also practised the same four rules, but they practised rules 3 and 4 through comprehension and rules 1 and 2 through production. A third group (n = 20) practised all four rules as well, but all of them through half comprehension and half production exercises. Thus, this third group engaged in mixed-modality practice of all rules, but they also got half the amount of practice for each modality. For all groups, comprehension practice was done by asking participants to match sentences they read on the computer with one out of four pictures. Production practice was done by asking them to type the sentence that would describe a given picture. Negative feedback on their answers to each item during practice was immediate and included explicit explanation of the error and showing the correct response.
All 15 practice sessions were divided into cycles of practice followed by testing. During the practice, all items were done half under normal single-task conditions and half under stressful dual-task conditions. In the latter dual-task condition, participants saw a number on the screen before an Autopractan practice item would appear; they then heard beeps at irregular intervals while doing the Autopractan practice item. After responding to the item, they had to subtract the number of beeps from the number that had appeared on the screen. The intention was to see whether the level of performance would substantially deteriorate initially (a sign that controlled processing was involved) and whether the stressor would become less obtrusive over time (a sign that automatic processing was taking over performance).
 
 

DeKeyser looked at evidence into both the process and the product of learning. In order to understand the learning process, reaction times and accuracy were plotted over the 15 testing events taken at the end of each practice session. He found that responses quickly became faster and more accurate over the ﬁrst two sessions, and that by practice session four or ﬁve performance had stabilized, with the speed and accuracy of responses remaining practically constant from that point on until the last session of practice. This pattern is predicted by the power law of learning. However, performance was essentially the same for the singleand dual-task condition items across all 15 sessions, which suggested that the distracting task in the dual condition had failed to sufﬁciently tax cognitive resources.
In order to document the product of learning, scores on the post-tests administered in the ﬁnal session of the study were also inspected. It turned out that gains were, as predicted, skill-speciﬁc. For a given rule, the participants in the ﬁrst two groups outperformed each other only on the items that tested them in the same modality in which they had practised that rule. By the same token, the balanced regime of comprehension and production experienced by the third group appeared to be effective for both comprehension and practice, with gains comparable to those made by the other two groups under the same-modality conditions.
As you see, testing the predictions of cognitive skill acquisition theory for L2 learning is complicated. Few studies have been conducted using this paradigm, and even fewer exist that document automatization over a sustained period of practice like DeKeyser (1997) did.
You may have noticed that a construct that underpins all the tenets and predictions of information processing is memory. Any information that our mind entertains, whether for milliseconds or for years, goes through memory and involves some form of memory. In the next sections, we examine some basic memory concepts and selectively look at some of the SLA insights they have spurred. Two types of memory are crucial in all cognitive operations: long-term memory and working memory. As we will see, both are fundamentally involved in second language processing and learning.

5.4	LONG-TERM MEMORY	

Long-term memory is about representation. It is virtually unlimited in its capacity and it is made of two kinds: explicit-declarative memory and implicit-procedural memory. Much of the knowledge encoded in long-term memory is explicit-declarative, that is, verbalizable and consciously recalled. Explicit-declarative memory supports recollection of facts or events, and it is served by the hippocampus in the human brain. As much knowledge, or probably more, is encoded in implicit-procedural memory. These are things that we know without knowing that we know them. Implicit-procedural memory supports skills and habit learning, and it is served by the neocortex in the human brain. About 35 years ago, Endel Tulving, an Estonian psychologist based in the United States, argued for the existence of another important difference in the kinds of knowledge stored in long-term memory and proposed a further distinction: semantic and episodic memory (see Tulving, 2002). Semantic memory pertains to relatively decontextualized knowledge of facts that ‘everyone knows’. Episodic memory involves knowledge of the events in which people are personally involved or ‘the events we’ve lived through’. Episodic memory corresponds to a more recent type of memory in evolution, believed to have evolved from semantic memory. It ‘allows people to consciously re-experience past experiences’ and also to think of their future (Tulving, 2002, p. 6).
Because vocabulary knowledge is one of the best-studied areas of long-term memory in SLA, we will look into it in order to exemplify the range of issues that arise regarding how new language knowledge is represented and stored.

5.5	LONG-TERM MEMORY AND L2 VOCABULARY KNOWLEDGE	

What does it mean to remember a word? At a fundamental level, a word is established in long-term memory when the link between a form and its meaning is made. However, knowing a word means a lot more: it includes the strength, size and depth of the knowledge represented in memory.
Vocabulary knowledge strength concerns the relative ability to use a given known word productively or to recognize it passively. Thus, strength is a matter of degree of proceduralization in implicit memory. Vocabulary strength has been the object of extensive investigation by Sima Paribakht and Marjorie Wesche in Canada (e.g. Paribakht and Wesche, 1997) and by Batia Laufer in Israel (e.g. Laufer and Goldstein, 2004). It is typically found that learners know more words receptively than productively, particularly if they are infrequent or difﬁcult words, and that this gap becomes smaller as proﬁciency develops. Within the purview of explicitdeclarative memory, by contrast, is the size of the mental lexicon, which refers to the total number of words known and represented in long-term memory. Size is often related to the relative frequency with which words are encountered in the input that surrounds learners, since high-frequency words usually make it into long-term memory earlier in the learning process than low-frequency words. Paul Nation in New Zealand has uncovered many interesting ﬁndings about vocabulary size in L2 learning (e.g. Nation and Waring, 1997; Nation, 2006). For example, in L1 a ﬁve-year-old child begins school with an established vocabulary of about 5,000 word families, and a typical 30-year-old college-educated adult ends up knowing about 20,000 word families (Nation and Waring, 1997). For L2 users, new vocabulary presents a formidable challenge. They need to learn about 3,000 new words in order to minimally follow conversations in the L2, and about 9,000 new word families if they want to be able to read novels or newspapers in the L2 (Nation, 2006).
Vocabulary depth resides in the realm of both explicit and implicit memory and refers to how well the known words are really known, that is, how elaborated, well speciﬁed and structured (or how analysed, in Bialystok’s 2001 sense) the lexical representations are. Among the most active contributors to the study of vocabulary depth are Paul Meara in Wales (e.g. Meara, 1996; Wilks and Meara, 2002) and Norbert Schmitt in England (e.g. Schmitt, 1998, 2000). Depth of knowledge includes whether L2 learners know how a word sounds (/di-‘zərt/ in a meal and
/‘de-zərt/ in a landscape), how it is spelled (exude, not exhude), how many other word parts it can appear with (pre–, –ment, –er, –s, –ing), what is likely to precede or follow a word (make a decision, do exercise; mental state, state of affairs/mind), how many meanings the word may have (demonstrate = to show and to protest), in what registers different synonyms may be preferred (weather, climate), or how frequently and in what contexts the word will occur (many oaths in court but not in hospitals, many incisions in hospitals but not in court). As Meara (2007) has argued for many years now, the notion of depth of vocabulary knowledge assumes the existence in implicit long-term memory of networks of meaning-based and formbased associations across the entire mental lexicon.
Other long-term memory-related questions in L2 vocabulary pertain to the content of the lexical representations encoded in memory for bilinguals. For example, Nan Jiang has initiated a research programme in which the long-term representation of L2 words is posited to be initially ﬁlled with the conceptual content from the L1 (e.g. Jiang, 2004). A different suggestion has been made by Finkbeiner et al. (2004), who raise the possibility that, until full competence is reached in both languages, L2 words have fewer meanings represented in their entries than L1 words, that is, L2 lexical representations are conceptually less elaborated and analysed. Focusing on episodic memory and conceptual development, Aneta Pavlenko (1999) has explored the hypothesis that L2 words learned in naturalistic contexts allow for the encoding of richer information in episodic memory than L2 words learned in classrooms, because the former are learned more experientially and the latter more declaratively. She therefore suggests that conceptual L2 development will be fostered in naturalistic (i.e. experiential) learning contexts but might be limited in foreign language contexts.
Finally, an important related question is how the L1 and L2 lexicons of the bilingual speaker interact, which is reﬂected in the mechanisms of access in long-term memory. Psycholinguists Judith Kroll (at Penn State University) and Annette de Groot (at the University of Amsterdam) have been proliﬁc contributors in this area (Kroll and de Groot, 1997). In their work they have shown that when bilinguals recognize or produce words, information encoded for both languages, not just the one of current use, is initially activated. This phenomenon is known as non-selectivity and has been documented also for three languages in trilinguals by Lemhfer et al. (2004). There is an interesting asymmetry, however. What is simultaneously activated in recognition is the L1-plus-L2 form representations, whereas in production what gets simultaneously activated initially is the L1-plusL2 meaning representations (Kroll et al., 2005).

5.6	WORKING MEMORY	

By contrast to long-term memory, which is about representation and is unlimited, working memory is about access and is limited. A simple but useful deﬁnition of working memory in SLA is offered via an example by Nick Ellis (2005): ‘If I ask you what 397  27 is, you do not look up the answer from long-term memory, you work it out’ (p. 338). Peter Robinson (1995) describes it as ‘the workspace where skill development begins … and where knowledge is encoded into (and retrieved from) long-term memory’ (p. 304). In other words, we need working memory to hold information (a storage function) as well as to integrate new information with known information already encoded in long-term memory (a processing function). Working memory handles automatic and controlled processing. Importantly, thus, it is the site for the executive control, which supports controlled processing (Baddeley and Hitch, 1974), and also the site of consciousness (Baars and Franklin, 2003). As Nick Ellis (2005) explains, working memory ‘is the home of explicit induction, hypothesis formation, analogical reasoning, prioritization, control, and decision-making. It is where we develop, apply, and hone our metalinguistic insights into an L2. Working memory is the system that concentrates across time, controlling attention in the face of distraction’ (p. 337).
Two characteristics help deﬁne working memory. First, unlike long-term memory, working memory is of limited capacity. Speciﬁcally, under normal conditions information can be remembered in working memory for about two seconds only. After that brief span, the representation is rapidly forgotten, unless it can be rehearsed subvocally in what Alan Baddeley, memory authority in the United Kingdom, called the phonological loop, so it can eventually enter long-term memory (Baddeley, 2007). A second characteristic is temporary activation. Activation is so central to working memory that Nelson Cowan, another main authority on L1 memory from the US, has obliterated the traditional distinction between long-term and working memory and suggests instead that working memory is just the part of memory that becomes activated during a processing event (Cowan, 2005).
Working memory has received intense attention in SLA since the mid-1990s. Perhaps its clearest application to SLA has been in the area of individual differences. The suggestion is that, since memory is involved in information processing in pervasive ways, people who have better working memory capacities can learn an L2 more efﬁciently. Thus, working memory capacity is posited to help predict learning rate and ultimate levels of attainment in the L2. We will examine these issues in Chapter 7 (sections 7.7 and 7.8). In terms of universal facts about L2 memory capacity, two observations have attracted some attention.
First, it has been observed that working memory capacity is smaller in the L2, when compared to the L1. For example, in one of the ﬁrst SLA studies of working memory, Harrington and Sawyer (1992) found that their 32 EFL participants’ memory performance was consistently lower in the L2 than in the L1 across a battery of memory tasks. More recently, Towell and Dewaele (2005) also found an L2–L1 lag, when the shadowing of a continuous oral passage was carried out in L2 and L1 by 12 participants, all students of French enrolled in secondor third-year levels at a university in the United Kingdom. As Vivian Cook (1996, p. 68) speculated at an early point in the investigation of L2 working memory, several mutually compatible explanations may account for this L2–L1 lag in capacity, each potentially related to a different component of working memory. It could be that the central executive functions less efﬁciently, because it has to work on L2 rather than L1 material. Or a slowdown in the capacity of the phonological loop to rehearse material subvocally could be at work, because one can articulate sounds in the L2 with less speed than in the L1. Or perhaps the interface between short-term memory and long-term memory is less efﬁcient because the rehearsed and remembered material is in the L2. Unfortunately, no research programme in SLA has attempted to tease out these possibilities thus far. Second, it has also been noted that, as L2 proﬁciency develops, this lag in working memory capacity between the L2 and L1 should become smaller. However, even less is known empirically about this widely held assumption. Without understanding better the ﬁrst question of how L2 working memory functions and what constellation of forces initially makes it smaller in capacity than L1 working memory, it will be difﬁcult to explore the second question of how L2 and L1 working memory capacity align together with increasing proﬁciency.

5.7	MEMORY AS STORAGE: PASSIVE WORKING MEMORY TASKS	

With memory, the ﬁrst thing that comes to mind is the simple notion of ‘storage size’, or just how much ﬂeeting information people can remember. This is actually the original scope of memory research in the L1, as reﬂected in the term short-term memory, a near-synonym choice with working memory. Enormous amounts of memory research have been devoted to the question of storage capacity in the L1, leading to the establishment of appropriate experimental tasks and interesting, if generic, benchmarks. These are brieﬂy presented in Table 5.1.
Although the limitation is understood traditionally in terms of sheer storage capacity, increasingly more L1 memory researchers have suggested alternative conceptualizations. In their inﬂuential model, Baddeley and Hitch (1974) explained limitations in terms of time passage, but more recently Cowan (2001) has proposed that the limits come from increasing confusion or interference with other similar material that enters the focus of attention. Yet other proposals by Ericsson and Kintsch (1995) and Logan (1988) have explained (from otherwise markedly different perspectives) limited capacity in terms of insufﬁcient relevant knowledge to draw from in long-term memory. Even more recently, Neil Burgess and Graham Hitch (2006) in the United Kingdom have proposed that short-term memory capacity is made of two distinct processes, one that enables memory for the content of the items themselves and the other that allows memory of serial position information within the items. Such ﬁne-grained explorations of the storage function of short-term memory have rarely been undertaken by SLA researchers (but see Speciale et al., 2004, for an exception). And yet, as Majerus et al. (2006) note, such explorations would be crucial to understand why memory capacity on tasks such as those listed in Table 5.1 are so good at predicting new L2 vocabulary learning, as we will see in Chapter 7 (section 7.8).
 

Table 5.1
Memory tasks and benchmarks in the study of storage memory capacity

Digit span recall tasks are one of the oldest methods used in psychological research to measure storage capacity. Put simply, participants are asked to repeat increasingly longer sequences of numbers, sometimes in backward order. Great care is taken to minimize opportunities for subvocal rehearsal, since this is a good strategy to ‘stretch’ memory capacity. As Miller (1956) concluded in a seminal paper, average-memory adults have a working memory span in their L1 of about seven digits, which means that they can remember sequences of about seven digits accurately at least 50% of the time
Word span tasks are also frequently used. Most people can repeat sequences of ﬁve to six unrelated words, but after that they experience increasing memory difﬁculties. Rather than thinking in terms of digits or words, however, it is more accurate to think of memory capacity in terms of chunks, or pieces of information that are already linked and stored together in long-term memory. For example, you will probably be able to remember equally well the string ‘Nicole, Gary, Tom, Katherine, Penélope, Sean’ and the double-length string ‘Nicole Kidman, Cary Grant, Tom Cruise, Katharine Hepburn, Penélope Cruz, Sean Connery’... that is, if you know these actors’ names well and have ‘chunked’ their ﬁrst and family names together for each of them
Non-word repetition span tasks are preferred to word span tasks by some researchers, precisely to eliminate recall-enhancing strategies that draw on long-term memory, like grouping and chunking. Examples of non-words are johmbe, zabide, wakime, migene, shosane, tisseke, chakume and nawase (taken from an L2 study by Williams, 2005) and lus, vip, kug, taysum, kepponen, woogalamic and reutterpation (taken from an L1 study by Gathercole et al., 1999)
Sentence repetition tasks (also called elicited imitation tasks) are another way to measure working memory storage capacity. We know that the typical human memory span for sentences is about 16 words, a sequence much longer than the typical word span of ﬁve or six isolated words. This is because in sentences we perceive words as ‘chunked’ into phrases. Our knowledge of grammar (which is stored in long-term memory) helps us group words into phrases and remember sentences better than isolated sequences of digits or words




5.8	MEMORY AS DYNAMIC PROCESSING: ACTIVE WORKING MEMORY TASKS

Working memory not only stores information, but it is also responsible for processing it. For this reason, L1 researchers have developed so-called active measures of working memory capacity that capitalize on the idea of a trade-off between storing and processing. This idea has become inﬂuential among L2 memory researchers through the work of cognitive psychologist Meredyth Daneman in Canada and her colleagues Patricia Carpenter and Marcel Just at Carnegie Mellon University (see Daneman and Merikle, 1996). They devised an active measure of working memory capacity in the L1 called the reading span task. In these tasks, people are asked to read sentences presented on cue cards and to comprehend or evaluate what they are reading. Immediately after reading all sentences, they are asked to recall the last word appearing on each sentence or the few selected words that were underlined on each card. This kind of reading span task is assumed to reﬂect how well people can maintain information in short-term memory (the targeted words they are asked to recall out of the sentence stimuli) while another processing task is executed simultaneously (reading the sentences for comprehension).
An interesting question in SLA is whether passive or active measures of working memory are better suited for the investigation of memory limits and L2 learning. Harrington and Sawyer (1992) argued that the predictive validity of active working memory measures was higher than that of passive measures, at least in their study. They found that the active measure of reading span scores predicted about 30 per cent of the variance in scores on the grammar and reading subsections of the TOEFL, whereas the passive measures of memory they used (digit and word span scores) did not correlate with any of these L2 proﬁciency measures. To some extent, however, the tasks yielding the correlations shared a heavy reliance on reading skills, and the reading span task was administered in the participants’ L2. A positive development is that more recent L2 research has employed measures of active working memory in the L1 (e.g. Sagarra, 2008).
In the end, there is no doubt that passive storage capacity alone is insufﬁcient to capture the varied contributions of memory to the acquisition of a second language. Cognitive psychologist Randall Engle (2002) argues that ‘[working memory] capacity is not about individual differences in how many items can be stored per se but about differences in the ability to control attention to maintain information in an active, quickly retrievable state. [It] is not directly about memory – it is about using attention to maintain or suppress information. Greater [working memory] capacity does mean that more items can be maintained as active, but this is a result of greater ability to control attention, not a larger memory store’ (p. 20). By conceptualizing memory as dynamic processing, such contemporary views of working memory have catapulted attention to the centre of cognition.

5.9	ATTENTION AND L2 LEARNING	

And, indeed, together with memory, attention is another essential component of cognition. Remember that under normal conditions simple activation of a stimulus in working memory will last for a few seconds and then fade away. Here is where attention comes in; it heightens the activation level of input in working memory, allowing it to remain there for longer through rehearsal and thus making it available for further processing and for entering long-term memory.
One main characteristic of attention is that its capacity is limited. You will note that this attribute was also mentioned when discussing working memory. These are all same-family metaphors. Working memory is capacity limited, possibly because attention is (Cowan, 2001). Because focal attention is limited, it is also thought to be selective. Only one attention-demanding processing task can be handled at the same time. Reﬂecting these selectivity effects, cognitive psychologists since the 1970s have contemplated metaphors that represent controlled attention as a bottleneck, a ﬁlter or a ﬂashlight. A third deﬁnitional feature is that attention can be voluntary, in the sense that it can be subject to cognitive, top-down control that is driven by goals and intentions of the individual. A fourth characteristic is that attention controls access to consciousness. Under normal conditions, participants can tell researchers about their conscious perceptions, thoughts or feelings while they attend to some aspect of a task, through the method of thinking out loud (Ericsson and Simon, 1993) or via some other retrospective method, as discussed in detail by Gass and Mackey (2000) in relation to SLA.
As we will see in the next ﬁve sections, the third and fourth deﬁnitional attributes of attention, voluntariness and consciousness, have driven SLA investigations into attention and L2 learning. How attention affects L2 learning has been examined by investigating the quality of attention that is necessary for L2 learning. The focus has been on processes and outcomes of learning under three attentional conditions, which can be summarized as: incidental (i.e. learning without intention, while doing something else), implicit (i.e. learning with no intervention of controlled attention, usually without providing rules and without asking to search for rules) and explicit (i.e. learning with the intervention of controlled attention, usually summoned by the provision of rules or by the requirement to search for rules). In a nutshell, SLA researchers have asked themselves whether L2 learning is possible without intention, without attention, without awareness and without rules.

5.10	LEARNING WITHOUT INTENTION	

Because attention can be voluntary, intentionality needs to be examined when evaluating what quality of attention is necessary for L2 learning. This is what we call the question of incidental L2 learning, which asks: Is it possible to learn about the L2 incidentally, as a consequence of doing something else in the L2, or does all L2 learning have to be intentional? It is unanimously agreed in SLA that incidental L2 learning is possible indeed. The learning of L2 vocabulary during pleasure reading is an incidental type of learning that has been found to be possible in the L2 as well as in the L1. This is what Jan Hulstijn (2003) concludes in a seminal review. This is also what the research reviewed by Krashen (2004), as well as the more recent evidence gleaned by Marlise Horst (2005) and others (e.g. Pigada and Schmitt, 2006), shows.
Nevertheless, lack of a priori intentions to learn while doing something in the L2 does not rule out the possibility that, in the course of processing, attention may be deliberately turned to the input. For example, while reading for pleasure and without any particular intention to learn vocabulary, an L2 reader may on her own suddenly become intent on ﬁnding out what a word means or may make a mental note to remember it and look it up later. In other words, we cannot ignore the fact that intentions wax and wane and ﬂuctuate during online processing and that, in the end, it is online attention that is at stake in a cognitive understanding of L2 learning. Furthermore, while learning without intention is possible, people learn faster, more and better when they deliberately apply themselves to learning. Thus, learning with intention remains of central importance in SLA because of its facilitative role, as Hulstijn and Laufer (2001) have argued with respect to vocabulary learning.

5.11	LEARNING WITHOUT ATTENTION	

Perhaps the most contested matter is whether new L2 material can be learned without attention, that is, if just detected pre-attentively. The debate has to do with Schmidt’s highly inﬂuential Noticing Hypothesis, which we introduced in Chapter 4, section 4.6. The question asked is: Is detection only sufﬁcient for L2 learning or is noticing necessary? Detection is deﬁned as registration outside focal or selective attention (Tomlin and Villa, 1994), whereas noticing is deﬁned as detection plus controlled activation into the focus of conscious attention (Schmidt, 1995).
Robinson (1995) and Doughty (2001) have argued that Nelson Cowan’s (1988, 2001, 2005) uniﬁed model of memory and attention offers a framework for envisioning this problem as one that depends on a continuum in the quality of attention (from low-level, automatic attention to high-level, controlled attention), rather than on an all-or-nothing dichotomy between unattended and attended processing. In Cowan’s model, detection that involves registration outside focal or selective attention is the kind of low-level, minimal attention typically assumed during automatic processing in information processing theories. For example, imagine we are taking a walk and our sensory storage catches a patch of green for several hundred milliseconds (Cowan, 1988). Immediate activation in long-term memory of an already existing representation that shares some fundamental feature will make us recognize it meaningfully, but pre-attentively, as a tree. This will occur without subjective experience, that is, without reaching consciousness. Any instance of language use mandatorily involves this kind of automatic, low-attentional processing (Ellis, 2002a). On the other hand, detection that goes on to involve focal or selective attention via controlled activation summons the kind of high-level, focal attention assumed during controlled processing in information processing theories. This quality of attention is thought to be accompanied by subjective experience or awareness at the time of processing. For example, we are taking a walk, our eyes catch a patch of green, we see a tree, but we also experience rapid feelings of vague pleasantness – maybe we even intuit that fall has begun. These are typical ﬂeeting consciousness effects that range according to US consciousness scholar Bernard Baars from fringe conscious (the vague pleasantness, the tacit memory of signs of fall around us) to more substantial and qualitative (visual imagery or inner speech) (Baars and Franklin, 2003). But we may immediately move on to something else and forget about having seeing any tree or having had this experience of ﬂeeting consciousness. Much of language use can also involve this kind of conscious, subjective awareness, while automatic, low-attentional processing also goes on (N. Ellis, 2002a, 2005).
 

Which of the two extreme qualities of attention (low-level automatic detection or high-level, controlled activation) leads to learning? Or can both result in learning? Herein lies the point of disagreement in SLA. Tomlin and Villa (1994) suggested, contra Schmidt, that detection at the periphery of focal attention is all that is necessary for L2 learning, whereas detection plus controlled activation into focal attention is facilitative of learning, but not necessary. Gass (1997) also agrees that noticing facilitates L2 learning but cannot be considered necessary. By contrast, Schmidt (1994, 2001) has maintained that detection involving peripheral attention is not enough for L2 learning, on the grounds that novel material that is attended peripherally could never be encoded in long-term memory. Instead, detection plus controlled activation into the focus of attention is needed for L2 learning: ‘what learners notice in input is what becomes intake for learning’ (1995, p. 20). Schmidt also proposes that nothing is free in L2 learning: ‘in order to acquire phonology, one must attend to phonology; in order to acquire pragmatics, one must attend to both linguistic forms and the relevant contextual features; and so forth’ (1995, p. 17).
Drawing on Cowan’s (1988) uniﬁed model of memory and attention, Robinson (1995) agreed with Schmidt that noticing is necessary for learning, but stipulated that noticing should be conceived as involving focal attention plus rehearsal, thus eschewing the vexing question of proving phenomenological awareness of the experience of noticing. Nick Ellis concedes that the Noticing Hypothesis may be right, but only if accompanied by an Implicit Tallying Hypothesis (2002a, p. 174), which imposes two provisos: (a) noticing is necessary only for new elements with certain properties that make low-attentional learning unlikely, but not for all aspects of language to be learned, and (b) noticing may be necessary only for the initial registration of such ‘difﬁcult’ elements so as to make an initial representation in long-term memory possible, but not for subsequent encounters. This is because ‘once a stimulus representation is ﬁrmly in existence, that stimulus … need never be noticed again; yet as long as it is attended to for use in the processing of future input for meaning, its strength will be incremented and its associations will be tallied and implicitly cataloged’ (Ellis, 2002a, p. 174). Acknowledging that it may be impossible to demonstrate zero noticing at the time of processing empirically (see discussion in section 5.12), Schmidt did shy away from his initial claim that noticing is the ‘necessary and sufﬁcient condition for converting input to intake’ (1990, p. 129), and since then his position has been that ‘more noticing leads to more learning’ (1994, p. 18). That is, noticing is facilitative of L2 learning (see also Schmidt, 2001).
In the end, then, the jury is still out on the question of whether learning without attention is possible. The real challenge for future research is to be able to specify empirically what investigating this question really entails in the context of processing for L2 learning.

5.12	LEARNING WITHOUT AWARENESS	

In its weaker form that states noticing is facilitative of L2 learning, the Noticing Hypothesis has attracted compelling support. Particularly the research programme led by Ron Leow at Georgetown University has offered ample evidence that noticing with awareness, and even more so with understanding, is facilitative of L2 learning. In these studies (e.g. Leow, 1997, 2001; Rosa and O’Neill, 1999; Rosa and Leow, 2004), think-aloud protocols are used to classify learners by their comments into: unaware, if no trace of noticing can be found in the introspective data; aware, if simple mention is made of the subjective experience of paying attention to the targets; or aware with understanding, if more abstract comments are made involving partial formulation of rules or generalizations. The results have consistently revealed better post-test scores for participants that produced verbal reports showing awareness and, even to a higher degree, understanding.
By contrast, in other research programmes where researchers have employed more delayed or indirect measures of awareness than think-alouds, a robust relationship between noticing and learning has not always been found. Thus, several interaction researchers (see Chapter 4, sections 4.7, 4.8 and 4.11) have indirectly measured noticing via uptake, or the incorporation of the interlocutor’s correction in the learner’s own utterance, and they have reported no relationship between uptake and score gains (Mackey and Philp, 1998; Loewen and Philp, 2006; McDonough and Mackey, 2006). Likewise, note taking as an indirect measure of noticing was also unrelated to post-test performance in at least one study (Izumi, 2002). It is unclear whether this pattern of results tells us something about the relationship between noticing and learning or about the validity of using uptake and note taking as awareness measures.

5.13	DISENTANGLING ATTENTION FROM AWARENESS?	

It should be emphasized that the Noticing Hypothesis posits that ‘learning requires awareness at the time of learning’ (Schmidt, 1995, p. 26, italics in the original). No claim is made that learners need to remain aware of what they noticed at any later point. Likewise, learner’s noticing does not need to include understanding of the nature of what they noticed; subsequent processes of hypothesis formation or rule abstraction are possible, but not a part of the noticing itself. It is therefore somewhat surprising that to date most SLA researchers have investigated the predictions of the Noticing Hypothesis by probing the presence or absence of self-reported, retrospective awareness. In doing so, SLA studies of noticing conducted since the late 1990s have entangled the question of controlled attention – Can there be learning resulting from low-level attention that is allocated outside the focal object of attention? – with a sequel question of awareness and consciousness – Can new L2 material be learned without awareness? (That is, Can there be a dissociation between L2 learning and awareness?)
A problem that is amply acknowledged in SLA is that demonstrating conclusively whether awareness was present or absent at the time of learning is methodologically impossible, because introspective and retrospective self-reporting is always imperfect. An additional problem that is rarely discussed, and one that is much more important from a theoretical perspective, is that the
strategies that allow us to measure the involvement of automatic vs controlled attention may be rather different from the strategies that help us measure awareness, and – if we accept Cowan’s reformulation of qualities of attention (see section 5.11) – the study of the former is more urgent than a pursuit of the latter.
UK-based cognitive psychologist David Shanks (2005), a sceptic of implicit learning, usefully explains the range of strategies that allow cognitive psychologists to measure the involvement of automatic-implicit attention versus strategies that help them measure awareness. Table 5.2 offers a summary of the main kinds of measures employed to bear on the issue of awareness in L2 learning, complemented with the insights offered by Shanks (2005) about measures of automatic-implicit attention.
In the end, then, in their zeal to shed light on the difﬁcult issue of awareness, SLA researchers have largely neglected the issue of the involvement of automatic vs controlled attention. Therefore, the various positions taken around the Noticing Hypothesis by Ellis (2002a), Robinson (1995), Schmidt (2001) and Tomlin and Villa (1994) remain empirically untested at their core.
 

Table 5.2
How can awareness versus automatic attention be measured in SLA studies?
AWARENESS
Self-report measures
People differ in how good they are at verbalizing explicit knowledge and the products of their awareness. Hence, positive reports are strong evidence of awareness, but absence of reports cannot be taken as evidence of zero awareness. See Gass and Mackey (2000) for further discussion
Concurrent reports
Thinking aloud while performing a task is the best window we have into the objects of consciousness, both fringe conscious awareness and more qualitative awareness at the time of processing (Baars and Franklin, 2003). For an example, see Leow (1997)
Retrospective reports
Debrieﬁng questionnaires, interviews, and stimulated recalls have also been used. In addition to the general problems of all verbal report measures, retrospection is ill-suited to capture ﬂeeting phenomenological awareness. One cannot discard the possibility that something was subjectively consciously attended to but the memory of being aware quickly faded away and was not available for recall at the later time when participants were asked. For examples, see Robinson (1997; questionnaire at the end of the study), Williams (2005, debrieﬁng interview at the end of the task), or Mackey et al. (2000; stimulated recall interviews after the task)
AUTOMATIC/IMPLICIT ATTENTION
Dual-task performance
As an alternative to self-report measures, near-zero-awareness states can be built into the experimental conditions themselves, via a dual competing or interference task paradigm. Shanks (2005) explains that if attention is consumed by a task and the learning targets are embedded in another task, it is assumed that any evinced learning of the targets falls outside focal attention and is without awareness. However, the doubt always remains of whether attention was actually entirely depleted by the demands of the competing task. Moreover, Shanks notes that this paradigm has generated conﬂicting results in psychology to date
Objective tests of memory after the training
This is another alternative to self-report, favoured by experimental psychology researchers. If targets processed outside focal awareness during training were learned, they should be more frequently recognized implicitly as ‘previously seen’ after the training. Objective memory tests, however, differ in sensitivity
Traditional direct memory tests
A set of items may be presented and participants are asked to recognize the ones that were involved in the training phase of the study, by selecting the ‘old’ items. For an example of this measure in SLA, see Shook (1994)
Indirect memory tests
Shanks (2005) suggests that indirect memory tests may offer improved alternatives to traditional recognition tests. Items are presented, one by one very brieﬂy, and participants are asked to identify the ones that they ‘prefer’ or ‘like’. Note that the experimenter’s question is not to make a judgement about novel versus old, but about liking. While the jury is still out in terms of whether cases of indirect recognition on these tests can be interpreted as cases of true implicit (unattended) learning, the evidence that these indirect tests elicit does seem more ﬁne-grained than that of traditional recognition tests. This kind of indirect memory test has not been used in SLA, although the logic of matched sentence tasks is similar. In these tasks, participants make decisions about whether sentences are similar, whereas the evidence that is inspected by the researcher is simply whether they take longer or shorter in making such judgements with grammatical or ungrammatical sentences (see Gass, 2001)
 
 


5.14	LEARNING WITHOUT RULES	

As explained in section 5.2, skill acquisition theorists would strongly argue against the wisdom of trying to begin the learning process without ﬁrst securing explicit declarative information about whatever new aspect of the L2 someone is trying to learn. Other SLA researchers interested in cognition, however, have a persistent interest in exploring the viability of implicit learning, or learning without rules. This interest is understandable if we remember that, when it comes to language, implicit (intuitive, tacit) learning has a special appeal. After all, children learn their L1 without any rules whatsoever, and many students experience a painful dissociation between what they ‘know’ in terms of rules and what they can ‘do’ with the L2 in real situations of use (we will return to this issue in Chapter 6, when we discuss the question of the interface in section 6.14).
At the heart of the SLA research programme on learning without rules is a focus on the products of implicit L2 learning: Can grammar generalizations result from experiencing L2 data without explicit knowledge being provided at the outset of the learning process? Or even without the learner actively and consciously searching to discover generalizations behind the language data she experiences?
United States psychologist Arthur Reber was the ﬁrst to expend sustained effort into the study of implicit learning, which he deﬁned as learning without rules. He pioneered an artiﬁcial grammar research paradigm in the 1960s that has been pursued by many others to this date (see Reber, 1996). In this type of experiment, participants in the implicit learning condition are asked to memorize strings of letters. This is an incidental-implicit learning condition in that: (a) participants think they are doing something (memorizing strings) that is different from what the researcher hopes they will do (extract formal regularities or rules), and (b) they are not given any explicit declarative knowledge (no rules for the artiﬁcial grammar) or any orienting towards the possibility of rules underlying the stimuli (no instructions to search for rules). When they are later asked to judge new strings as grammatical or not, they perform above chance level. This is interpreted as proof that they learned something about the artiﬁcial grammar indeed. When requested to verbalize any rules at all, however, they are at a loss. This is interpreted as evidence that their learning has resulted in implicit (intuitive, non-verbalizable) knowledge of the artiﬁcial grammar.
An important point of contention in interpreting these results, however, is whether learning without rules is about symbolic or associative learning. For those who, with Reber, believe we can learn without rules or awareness of rules, the proposal is that implicit (unconscious) processing leads to the abstraction of rules that are symbolically represented in the mind, only that they happen to be inaccessible to consciousness. That is, their theories of implicit learning are abstractionist and symbolic. However, increasingly more psychologists are willing to reinterpret the evidence from implicit learning studies as showing learning of underlying statistical structure, rather than learning of underlying rules (Shanks, 2005). This radical proposal has been made possible by the appearance and burgeoning of connectionist and associative theories in psychology since the 1980s (to be discussed in the last section of this chapter).
Returning to L2 learning matters, what would happen if no explicit information were made available to L2 learners at the outset of training? Would automatization look different from learning that begins with declarative rules, as in the study by DeKeyser (1997) we examined in section 5.3? And what would the resulting representations be like, since there is no declarative rule to be proceduralized? Would L2 learners abstract their own rules from the input they work on? Or would they show evidence of associative memories only? Robinson (1997) set out to investigate just these questions.

5.15	AN EXEMPLARY STUDY OF SYMBOLIC VS. ASSOCIATIVE LEARNING: ROBINSON (1997)	

Robinson (1997) investigated whether 60 Japanese college students would learn an English grammar rule better without or with rules, after 25 minutes of training. Robinson focused on the products of automatization, deﬁned as the accuracy and speed of response on an immediate post-test after the 25-minute treatment.
The target rule chosen was the English dative alternation, or rather a small aspect of it, which was of reasonable difﬁculty for the Japanese college students in this study. Consider the two sentences:

(1)	John gave the cake to Mary
John donated the piano to the church

The verbs give and donate have close meanings in the above two sentences. So, why is it that we can choose the alternative option John gave Mary the cake but not *John donated the church the piano? Many speakers of English, including of course ﬁrst language speakers, are surprised to learn that give can alternate between the two options because it is an Anglo-Saxon verb, whereas Latinate verbs (e.g. donate) do not allow this word order alternative (there are several other, more abstract ways in which the alternation is motivated, explained in Pinker, 1989). In pedagogical terms, if a rule of thumb were to be given, we would tell English students that onesyllable verbs can take both word orders, whereas bior multiple-syllable verbs (because Latinate verbs always have at least two syllables) can only be well formed with the ‘to-phrase’. Robinson decided to use invented verbs rather than real ones, to circumvent the possibility that some participants would beneﬁt from their existing memory of how commonly encountered verbs (e.g. give) work. Some examples of multisyllabic verb items used in the training and tests were:

(2)	Nick menided some hot coffee to Sue
*Sandy bivarded Patrick some Swiss cake

You see how challenging it must have been for the participants to work through these sentences and to end up with some kind of ability to judge the grammaticality of test items after 25 minutes. For the researcher the challenge was to answer the following question: Would the participants under the different conditions end up making grammaticality judgements by relying on knowledge of the monosyllabic versus disyllabic rule for when to use the to-phrase? Or would they make their judgements aided by a stock of memorized instances (the ones they encountered in the 25-minute session)?
For the implicit group, the task required that participants remember the position of words in sentences (a modiﬁcation of the implicit learning condition to memorize artiﬁcial grammar strings used by Reber). The incidental group read the sentences for meaning and answered comprehension questions (a condition that resembles Krashen’s comprehensible input; see Chapter 4, section 4.3). These two groups would encounter the verbs and the syntax of those sentences but likely process the items without any awareness of a ‘rule’ being embedded in them. Consequently, Robinson predicted they would learn a stock of instances rather than an abstract rule. That is, their responses would be fast, but accuracy would be possible only for sentences already encountered in the training, and not for novel sentences that were also included in the post-tests. Two other groups in the study experienced the items under explicit learning conditions of two kinds. The participants in the less explicit of the explicit groups saw the input typographically enhanced and were encouraged to ﬁnd out the underlying rule. The participants in the more explicit of the explicit groups received an explanation of the monosyllabic versus multisyllabic rule, followed by practice. This condition, therefore, was closest to the ideal in skill acquisition theory (see sections 5.2 and 5.3). The prediction for these two explicit conditions was that training would lead to abstract generalizations, precisely because the instructions asked these participants to try to discover or to use rules, respectively. They would be able to apply rules, perhaps in a slower fashion, but they would be more accurate with new sentences during testing, because they would be able to ‘generalize’ what they learned beyond the speciﬁc items they worked with during training.
The study results supported these predictions for the most part. Interestingly, however, the instructed (skill-acquisition-like) group outperformed all other three groups, as they were faster and more accurate on both old and new sentences. But all four groups also showed the same effect regarding a relatively faster reaction for already encountered items, as if the memory of those actual instances encountered during training had also contributed to learning, not just the conceptual guide of a rule. Robinson interpreted this as evidence that effects coexist from conceptually driven abstraction and data-driven, memory-based learning, even in the instructed condition.
In the end, then, is L2 learning possible without rules? Robinson concludes that, in the absence of rules, low-level associative learning that draws on data-driven processes supported by memory is certainly possible. Learning without rules leads to the formation of memories of instances that can be accessed more easily, allowing for faster performance, but without knowledge that can be generalized to new instances. That is, without the initial provision of rules (without an explicit learning condition), learning is bottom-up (i.e. data and memory driven), and it does not lead to knowledge of a systematic rule of some kind. With rules, learning proceeds by drawing on high-level attention and conceptually driven processes supported by conscious attention, resulting in generalization with awareness.
Robinson (1997), as well as Nick Ellis (2005) and John Williams (1999), have argued that in the future debates about implicit learning must be recast in terms of the interaction between low-level associative learning that draws on data-driven processes supported by memory and high-level cognitive learning that draws on conceptually driven processes supported by conscious attention. Both types of processing for learning can occur and both can interact. A pending question for future research is whether all aspects of an L2 are equally learnable by implicit means or whether some particularly complex aspects of the L2 may require conceptually driven processing in order for associations and representations to be formed (Ellis, 2002a, p. 174).
The reconceptualization of implicit learning as statistical learning is just one of many consequences of a wider trend in cognitive psychology to reconceptualize information processing as an associative, probabilistic, rational, usage-based, grounded, dynamic and, in sum, emergent adaptation of the agent to the environment. We ﬁnish this chapter with a forward-looking bird’s-eye view of this trend.

5.16	AN EMERGENTIST TURN IN SLA?	

In the last few years, what can be characterized as an imminent emergentist turn has made inroads into several research programmes about cognition in SLA. Emergentism refers to a contemporary family of theories in cognitive science that have coalesced out of increasingly critical examinations of the tenets of information processing theories. Because they have evolved out of information processing, they share much common ground with it. However, because emergentism is critical of traditional cognitivist notions and goes well beyond them, it is sometimes characterized as a post-cognitivist paradigm (Potter, 2000; Wallace et al., 2007). In the ﬁeld of SLA the pioneer of emergentism has been language psychologist Nick Ellis. For over a decade now, he has led other SLA scholars to think about the associative, probabilistic and usage-based nature of L2 acquisition, ﬁrst from the University of Wales Bangor (e.g. Ellis, 1993, 1996) and currently from the University of Michigan (e.g. Ellis and Larsen-Freeman, 2006; Ellis, 2007; Robinson and Ellis, 2008b).
In a manifesto of emergentism in SLA, Ellis and Larsen-Freeman summarized the position as follows:

Emergentists believe that simple learning mechanisms, operating in and across the human systems for perception, motor-action and cognition as they are exposed to language data as part of a communicatively-rich human social environment by an organism eager to exploit the functionality of language, sufﬁce to drive the emergence of complex language representations.
(2006, p. 577)

The emergentist family of explanations for L2 learning is built on several nested assumptions that borrow from diverse contemporary schools in cognitive science, all sharing a post-cognitivist bent. Three important tenets on which emergentist approaches build are associative learning, probabilistic learning and rational contingency (Ellis, 2006a). From these three principles derive the ‘simple learning mechanisms’ to which Ellis and Larsen-Freeman (2006) refer in the quote above.
Associative learning, as we saw in section 5.15, means that learning happens as we form memories of instances or exemplars we experience in the input, in a process of automatic extraction of statistical information about the frequency and sequential properties of such instances. Ellis (2006a) explains that the human architecture of the brain is neurobiologically programmed to be sensitive to the statistical properties of the input and to learn from them. When processing stimuli, the brain engages in a continuous and mandatory (as well as implicit, in the sense of automatic and certainly unconscious) tally of overall frequency of each form and likelihood of co-occurrence with other forms. This statistical tallying is supported by neural structures in the neocortex (Ellis, 2006b). Probabilistic learning posits that learning is not categorical but graded and stochastic, that is, it proceeds by (subconscious) guesswork and inferences in response to experience that always involves ambiguity and uncertainty (Chater and Manning, 2006). However, this kind of probabilistic calculation is not a slave of whatever is experienced by the human brain as a contiguous temporal or spatial surface pattern. Instead, the probability calculations of the human mind are guided by principles of rational contingency, or automatically computed expectations of outcomes on the basis of best possible evidence (Chater and Manning, 2006; Ellis, 2006b). Speciﬁcally, the processor makes best-evidence predictions about outcomes based upon (a) the overall statistics extracted by accumulated experience, (b) the most recent relevant evidence, (c) attention to cues detected to be present and (d) the clues provided by the context (Ellis, 2002a, 2006a, 2006b, 2007). Each time the outcome is conﬁrmed or not in another relevant event, the processor adjusts to the new evidence and modiﬁes its prediction so its predictive accuracy is better next time.
Additional important tenets in the emergentist family of theories are perhaps broader in scope. One is usage-based learning, or the position that language use and language knowledge are inseparable, because we come to know language from using it. Hence the speciﬁcation in the earlier quote by Ellis and Larsen-Freeman (2006) that learning from exposure comes about ‘as part of a communicatively-rich human social environment’ and is experienced ‘by an organism eager to exploit the functionality of language’ (p. 577). Among others, US cognitive scientist Michael Tomasello, now at the Max Planck Institute in Germany, has been instrumental in advancing a view of language acquisition that is usage-based, where grammar concepts emerge out of communicative and social needs: ‘people construct relational and semantic categories in order to make sense of the world and in order to communicate with one another’ (Abbot-Smith and Tomasello, 2006, p. 282). Importantly, this commitment to usage-based learning means that two traditional distinctions in linguistics and information processing, respectively, are transcended: competence and performance, and representation and access. Furthermore, meaning (rather than rules) is held to be of primary importance in understanding the language faculty. For this reason, the linguistic schools that best suit the emergentist project are cognitive linguistics (Langacker, 2008) and corpus linguistics (Gries, 2008).
Another broad-scope tenet of emergentism is that cognition is grounded, and therefore language is too. By this, it is meant that our species’ experience in the world and the knowledge that we abstract from such experience is always structured by human bodies and neurological functions (Evans et al., 2007; Barsalou, 2008). This is why Ellis and Larsen-Freeman (2006) described learning mechanisms as ‘operating in and across the human systems for perception, motor-action and cognition’ (p. 577). Perception and action, and not only abstract or symbolic information, are believed to shape cognition (Wilson, 2002). Hence, perceptual and sensory-motor functions of the brain must also be implicated in language acquisition. They contribute to the emergence of language abstractions and they also constrain and guide many of the simple learning mechanisms of associative, probabilistic and rational contingent learning.
The ﬁnal tenet that is worth highlighting in this synoptic examination of emergentism is that language acquisition, like the acquisition of other forms of cognition, is a self-organizing dynamical system. This entails viewing the phenomenon to be explained (e.g. language learning) as a system (or ecology) composed of many interconnected parts that self-organize on the basis of multiple inﬂuences outside the system; these inﬂuences provide constraints that afford self-organization, but no single cause has priority over others, as explicated in developmental psychology by Linda Smith and the late Esther Thelen at Indiana University (Smith and Thelen, 2003) and by Paul van Geert (1998) at the University of Groningen. In addition, a change in any given part of the system will result in changes in other parts, but the two need not be commensurate in size or importance. As Smith and Thelen describe, ‘Development can be envisioned, then, as a series of evolving and dissolving patterns of varying dynamic stability, rather than an inevitable march towards maturity’ (p. 344). In SLA, an early call of attention to dynamical systems was made by Diane Larsen-Freeman (1997), and a recent research programme has been more explicitly proposed by Kees de Bot and his colleagues at the University of Groningen (de Bot et al., 2007; de Bot, 2008). Dynamicity in self-organizing systems also places variability at the heart of investigation (de Bot et al., 2007). Much of the theory behind the tenet of dynamic systems originated from insights in meteorology sciences (Larsen-Freeman, 1997). It is no wonder, then, that Ellis (2002a) comments: ‘The multiplicity of interacting elements in any system that nontrivially represents language makes the prediction of the patterns that will eventually emerge as difﬁcult as forecasting the weather, the evolution of an ecological system, or the outcome of any other complex system’ (p. 178).
It would also be an uncertain weather forecast exercise to predict how long it will take for usage-based, dynamic systems, emergentist SLA to really come to fruition. Certainly, emergentist thinking about L2 learning has been vibrant, but the consequences have been less noticeable in terms of empirical activity. Thus, for instance, the implications of connectionism for L2 learning have been discussed frequently, beginning in the 1990s (Ellis, 1998; Gasser, 1990), but empirical applications of connectionism to SLA have been rare (e.g. Ellis and Schmidt, 1998). Full empirical deployment is gradually on the rise, however, and may become a reality in the not-so-distant future, judging from the impressive array of not only theoretical but also empirical knowledge around cognitive linguistics and emergentist SLA that Robinson and Ellis (2008a) have gathered. However long it takes, what is certain is that emergentism in SLA will ﬂourish, as it has in a number of other ﬁelds.
Second language learning under an emergentist perspective has the potential to look less like development that proceeds teleologically towards the ultimate attainment of a so-called native grammar, and more like a complex deployment of human multi-language capacities as a function of experience in the world. The picture of L2 development they offer resonates with what we know about learner language, which we will discuss in the next chapter. The new approach, by redeﬁning cognition as emergent, helps envision additional language acquisition less as a formal, deterministic and symbolic feat and more as an ecological phenomenon, ‘a dynamic process in which regularities and system emerge from the interaction of people, their conscious selves, and their brains, using language in their societies, cultures, and world’ (Ellis, 2007, p. 85).